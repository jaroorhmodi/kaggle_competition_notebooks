{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This notebook is meant to be run on kaggle as part of kaggle competitions.\nimport os\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T03:45:21.026478Z","iopub.execute_input":"2021-08-12T03:45:21.026899Z","iopub.status.idle":"2021-08-12T03:45:21.030715Z","shell.execute_reply.started":"2021-08-12T03:45:21.026861Z","shell.execute_reply":"2021-08-12T03:45:21.029874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports and Loading Dataset\n\nImporting valuable modules and place dataset in GCS bucket to be usable by TPU.\n","metadata":{}},{"cell_type":"code","source":"import math, re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# checking the version of tf we are using\nprint('tensorflow version  ' + tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:21.074025Z","iopub.execute_input":"2021-08-12T03:45:21.074375Z","iopub.status.idle":"2021-08-12T03:45:21.081191Z","shell.execute_reply.started":"2021-08-12T03:45:21.074344Z","shell.execute_reply":"2021-08-12T03:45:21.079925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load competition data\nfrom kaggle_datasets import KaggleDatasets\n\n# Add your data into your input directory in Kaggle\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(GCS_DS_PATH) #places the dataset in a Google Cloud Storage bucket for TPU usage","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:21.125974Z","iopub.execute_input":"2021-08-12T03:45:21.126329Z","iopub.status.idle":"2021-08-12T03:45:21.520838Z","shell.execute_reply.started":"2021-08-12T03:45:21.12629Z","shell.execute_reply":"2021-08-12T03:45:21.520031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPUs and Distribution Strategy\n\nGetting a distribution strategy for calculation on TPUs. TPUs have 8 cores, each of which work sort of like a GPU. We will make tensorflow use all of these cores at once using a distribution strategy.","metadata":{}},{"cell_type":"code","source":"# Try to detect a TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master()) #this works on kaggle because Kaggle defines TPU_NAME in the environment variables\nexcept ValueError:\n    tpu = None\n\n# If we have a TPU, use a distribution strategy for it, if not let's just use a normal distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)#connect to tpu cluster\n    tf.tpu.experimental.initialize_tpu_system(tpu)#initialize our cluster\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint('REPLICAS: {0}'.format(str(strategy.num_replicas_in_sync)))\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-12T03:45:21.522185Z","iopub.execute_input":"2021-08-12T03:45:21.522718Z","iopub.status.idle":"2021-08-12T03:45:28.937372Z","shell.execute_reply.started":"2021-08-12T03:45:21.522684Z","shell.execute_reply":"2021-08-12T03:45:28.936311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data for Processing\n\nWe need to set up functions to get images from tfrecords and load them into our dataset.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# we need image size and path setups for different sizes of images\n# IMAGE_SIZE = [192, 192]\nIMAGE_SIZE = [224, 224]\n# IMAGE_SIZE = [331, 331]\n# IMAGE_SIZE = [512, 512]\nGCS_PATH = GCS_DS_PATH + '/tfrecords-jpeg-{0}x{1}'.format(str(IMAGE_SIZE[0]),str(IMAGE_SIZE[1])) #this autoselects the correct input path\nprint('GCS_PATH = ' + GCS_PATH)\nAUTO = tf.data.experimental.AUTOTUNE #not sure what this does, I am trying to find information on this from tf documentation\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')\n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32)/255.0 #converts rgb values to floats in [0,1]\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) #reshapes image to an [IMAGE_SIZE[0], IMAGE_SIZE[1], 3] tensor\n    return image\n\ndef read_labeled_tfrecord(example):\n    #pass in a format for tf.io.parse_single_example\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string), #tf.string is a bytestring\n        'class': tf.io.FixedLenFeature([], tf.int64),  #shape[] refers to there being only a single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    #pass in a format for tf.io.parse_single_example\n    UNLABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'id': tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    #Reads tfrecs. Optimal performance when you do not care for data order.\n    #Therefore, we provide the option but default to false for data ordering.\n    #We will be shuffling data anyway so it shouldn't make a difference.\n    options = tf.data.Options()\n    if not ordered:\n        options.experimental_deterministic = False #disable ordering when reading in to increase speed\n    \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) #this is where our AUTOTUNE param is used to auto-adjust parallel reads\n    dataset = dataset.with_options(options)\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO)\n    #we map the correct reading function onto our dataset. AUTOTUNE is used here to adjust the parallelism of the calls on the dataset.\n    return dataset\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T03:45:28.939127Z","iopub.execute_input":"2021-08-12T03:45:28.939473Z","iopub.status.idle":"2021-08-12T03:45:29.158527Z","shell.execute_reply.started":"2021-08-12T03:45:28.939438Z","shell.execute_reply":"2021-08-12T03:45:29.157355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Data Pipelines\nThe goal is to create efficient pipelines for train/test/val splits.\n","metadata":{}},{"cell_type":"code","source":"# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\ndef data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO)\n    # statement in the next function (below), this happens essentially\n    # for free on TPU. Data pipeline code is executed on the \"CPU\"\n    # part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    #image = tf.image.random_saturation(image, 0, 2)\n    return image, label   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec\n    # files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:29.161163Z","iopub.execute_input":"2021-08-12T03:45:29.161483Z","iopub.status.idle":"2021-08-12T03:45:29.174075Z","shell.execute_reply.started":"2021-08-12T03:45:29.161452Z","shell.execute_reply":"2021-08-12T03:45:29.172923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will call these functions to create the datasets that we will be using for training. ","metadata":{}},{"cell_type":"code","source":"ds_train = get_training_dataset()\nds_valid = get_validation_dataset(ordered=True)\nds_test = get_test_dataset()\n\nprint(\"Training:\", ds_train)\nprint (\"Validation:\", ds_valid)\nprint(\"Test:\", ds_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:29.175975Z","iopub.execute_input":"2021-08-12T03:45:29.176446Z","iopub.status.idle":"2021-08-12T03:45:29.443053Z","shell.execute_reply.started":"2021-08-12T03:45:29.176376Z","shell.execute_reply":"2021-08-12T03:45:29.441806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These datasets are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. The training and validation sets are streams of `(image, label)` pairs.","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(threshold=15, linewidth=80)\n\nprint(\"Training data shapes:\")\nfor image, label in ds_train.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:29.446283Z","iopub.execute_input":"2021-08-12T03:45:29.446638Z","iopub.status.idle":"2021-08-12T03:45:30.503573Z","shell.execute_reply.started":"2021-08-12T03:45:29.446603Z","shell.execute_reply":"2021-08-12T03:45:30.502558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set is a stream of `(image, idnum)` pairs; `idnum` here is the unique identifier given to the image that we'll use later when we make our submission as a `csv` file.","metadata":{}},{"cell_type":"code","source":"print(\"Test data shapes:\")\nfor image, idnum in ds_test.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:30.505125Z","iopub.execute_input":"2021-08-12T03:45:30.505759Z","iopub.status.idle":"2021-08-12T03:45:31.387567Z","shell.execute_reply.started":"2021-08-12T03:45:30.505707Z","shell.execute_reply":"2021-08-12T03:45:31.386476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    np_images = images.numpy()\n    np_labels = labels.numpy()\n    if np_labels.dtype == object:\n        np_labels = [None for _ in enumerate(np_images)]\n    #this ensures we have None as the label in test data\n    return np_images, np_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], TRUE #CLASSES is the array we used earlier to index flower labels\n    correct = (label == correct_label)\n    return '{0} [{1}{2}{3}]'.format(CLASSES[label], 'OK' if correct else 'NO', u'\\u2192' if not correct else '', CLASSES['correct_label'] if not correct else ''), correct\n\n#pyplot visualization functions to show images of flowers\ndef display_one_flower(image, title, subplot, red = False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch, predictions=None):\n    # This works with:\n    # display_batch_of_images(images)\n    # display_batch_of_images(images, predictions)\n    # display_batch_of_images((images, labels))\n    # display_batch_of_images((images, labels), predictions)\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square\n    # or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10 == 1:\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:31.390695Z","iopub.execute_input":"2021-08-12T03:45:31.39125Z","iopub.status.idle":"2021-08-12T03:45:31.412978Z","shell.execute_reply.started":"2021-08-12T03:45:31.391212Z","shell.execute_reply":"2021-08-12T03:45:31.41182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Important Tip\nYou can use the below cell to get an iterator over the dataset.","metadata":{}},{"cell_type":"code","source":"#this will give you batches of size 20 every time you iterate\nds_iter = iter(ds_train.unbatch().batch(20))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:31.414901Z","iopub.execute_input":"2021-08-12T03:45:31.415195Z","iopub.status.idle":"2021-08-12T03:45:31.445849Z","shell.execute_reply.started":"2021-08-12T03:45:31.415165Z","shell.execute_reply":"2021-08-12T03:45:31.444686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#you only have to rerun this cell to iterate over the batch and display images per batch\none_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:31.4473Z","iopub.execute_input":"2021-08-12T03:45:31.447629Z","iopub.status.idle":"2021-08-12T03:45:33.191568Z","shell.execute_reply.started":"2021-08-12T03:45:31.447597Z","shell.execute_reply":"2021-08-12T03:45:33.190516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduling a Learning Rate\nTo schedule a learning rate we need to create a function which returns function which returns a learning rate for each epoch. We pass this into keras using `keras.callbacks.LearningRateScheduler` to be able to use it in model fitting.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 20\n# Our Learning Rate schedule is defined here:\ndef exponential_lr(epoch,\n                   start_lr = 0.0001, min_lr = 0.00001, max_lr = 0.0001, \n                   rampup_epochs = 1, sustain_epochs = 1, exp_decay = 0.8):\n\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n        # linear increase from start to rampup_epochs\n        if epoch < rampup_epochs:\n            lr = ((max_lr - start_lr) /\n                  rampup_epochs * epoch + start_lr)\n        # constant max_lr during sustain_epochs\n        elif epoch < rampup_epochs + sustain_epochs:\n            lr = max_lr\n        # exponential decay towards min_lr\n        else:\n            lr = ((max_lr - min_lr) *\n                  exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                  min_lr)\n        return lr\n    return lr(epoch,\n              start_lr,\n              min_lr,\n              max_lr,\n              rampup_epochs,\n              sustain_epochs,\n              exp_decay)\n\n# def lr_scheduler(epoch):\n#     if epoch < 4:\n#         return 0.0005\n#     elif epoch < 8:\n#         return 0.0002\n#     elif epoch < 12:\n#         return 0.0001\n#     elif epoch < 16:\n#         return 0.00005\n#     elif epoch < 20:\n#         return 0.00002\n#     else:\n#         return 0.00001\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)\n\n\n\nrng = [i for i in range(EPOCHS)]\ny = [exponential_lr(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:33.192959Z","iopub.execute_input":"2021-08-12T03:45:33.193483Z","iopub.status.idle":"2021-08-12T03:45:33.362682Z","shell.execute_reply.started":"2021-08-12T03:45:33.193443Z","shell.execute_reply":"2021-08-12T03:45:33.361639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Model\n\nWe can use a pretrained model to use **transfer learning**. This will improve the time it takes to train the model.\n\nWe are using **VGG16**, **DenseNet201**, **EfficientNetB7**, and **Xception** as components of our ensemble.\n\nI learned a method for this from [**this awesome notebook**](https://www.kaggle.com/servietsky/pretrained-cnn-epic-fight/notebook#4.-Pretrained-Models-Creation-). I highly encourage anyone reading this to check it out.\n\nWe will employ a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with) which we get from `strategy.scope`. We defined `strategy` at the beginning of the notebook. The context manager tells TensorFlow hoiw to divide tasks between the TPU cores. When using tensorflow with a TPU you will need to use this concept of a context to run your training in.","metadata":{}},{"cell_type":"code","source":"# EPOCHS = 20 #this number can be changed to be whatever you like\n\n# We are including a drop_rate argument here, setting it to 0 should give us an effect\n# equivalent to having no dropout layer at all. This is just to make experimentation easier\n# and cleaner. We allow selection of an optimizer (but since these are CNNs, nadam or adam are best).\n\ndef model_VGG16(drop_rate = 0.2, opt = 'nadam', trainable_arg = False):\n    with strategy.scope():\n        pretrained_model = tf.keras.applications.VGG16(\n            weights='imagenet',\n            include_top=False,\n            input_shape=[*IMAGE_SIZE, 3]#same shape as we pass out of decode_image\n        )\n        pretrained_model.trainable = trainable_arg #this will make the layers of the pretrained model untrainable\n        #of course, this does not mean the added layers will not be able to learn in our model.\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(), #this layer attaches behind our VGG16 pretrained as a classifier\n            tf.keras.layers.Dropout(drop_rate),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax', dtype='float32') #output layer, provides classification\n        ])\n    model.compile(\n        optimizer = opt,\n        loss = 'sparse_categorical_crossentropy',\n        metrics = ['sparse_categorical_accuracy']\n    )\n    return model\n    \ndef model_Xception(drop_rate = 0.2, opt = 'nadam', trainable_arg = False):\n    with strategy.scope():\n        pretrained_model = tf.keras.applications.Xception(\n            weights='imagenet',\n            include_top=False,\n            input_shape=[*IMAGE_SIZE, 3]#same shape as we pass out of decode_image\n        )\n        pretrained_model.trainable = trainable_arg #this will make the layers of the pretrained model untrainable\n        #of course, this does not mean the added layers will not be able to learn in our model.\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(), #this layer attaches behind our VGG16 pretrained as a classifier\n            tf.keras.layers.Dropout(drop_rate),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax', dtype='float32') #output layer, provides classification\n        ])\n    model.compile(\n        optimizer = opt,\n        loss = 'sparse_categorical_crossentropy',\n        metrics = ['sparse_categorical_accuracy']\n    )\n    return model\n\ndef model_densenet(drop_rate = 0.2, opt = 'nadam', trainable_arg = False):\n    with strategy.scope():\n        pretrained_model = tf.keras.applications.DenseNet201(\n            weights='imagenet',\n            include_top=False,\n            input_shape=[*IMAGE_SIZE, 3]#same shape as we pass out of decode_image\n        )\n        pretrained_model.trainable = trainable_arg #this will make the layers of the pretrained model untrainable\n        #of course, this does not mean the added layers will not be able to learn in our model.\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(), #this layer attaches behind our VGG16 pretrained as a classifier\n            tf.keras.layers.Dropout(drop_rate),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax', dtype='float32') #output layer, provides classification\n        ])\n    model.compile(\n        optimizer = opt,\n        loss = 'sparse_categorical_crossentropy',\n        metrics = ['sparse_categorical_accuracy']\n    )\n    return model\n\ndef model_effnet_B7(drop_rate = 0.2, opt = 'nadam', trainable_arg = False):\n    with strategy.scope():\n        pretrained_model = tf.keras.applications.DenseNet201(\n            weights='imagenet',\n            include_top=False,\n            input_shape=[*IMAGE_SIZE, 3]#same shape as we pass out of decode_image\n        )\n        pretrained_model.trainable = trainable_arg #this will make the layers of the pretrained model untrainable\n        #of course, this does not mean the added layers will not be able to learn in our model.\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(), #this layer attaches behind our VGG16 pretrained as a classifier\n            tf.keras.layers.Dropout(drop_rate),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax', dtype='float32') #output layer, provides classification\n        ])\n    model.compile(\n        optimizer = opt,\n        loss = 'sparse_categorical_crossentropy',\n        metrics = ['sparse_categorical_accuracy']\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:33.363941Z","iopub.execute_input":"2021-08-12T03:45:33.364227Z","iopub.status.idle":"2021-08-12T03:45:33.383039Z","shell.execute_reply.started":"2021-08-12T03:45:33.364199Z","shell.execute_reply":"2021-08-12T03:45:33.382069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DRATE = 0.5 #this is just a default value, started at 0.1 but it was overfitting\nOPT = 'adam' #this was a good optimizer for VGG16, Xception\nTARG = True #make models trainable\n\n\n# 'VGG16': model_VGG16(drop_rate = DRATE, opt = OPT, trainable_arg = TARG),\n# removed VGG16 altogether.\n\nmodels = {\n          'Xception':model_Xception(drop_rate = DRATE, opt = OPT, trainable_arg = TARG),\n          'DenseNet201':model_densenet(drop_rate = DRATE, opt = OPT, trainable_arg = TARG),\n          'EfficientNetB7':model_effnet_B7(drop_rate = DRATE, opt = OPT, trainable_arg = TARG)\n         }\n#testing why my validation accuracy is so high for first epochs of the last three models\n#running just one model to see if I get something like 60% validation accuracy for the first epoch\n#[[[AFTER TESTING: OK THIS IS FINE, THE OTHER MODELS JUST START WITH QUITE HIGH VALIDATION ACCURACY COMPARED TO VGG16]]]\n\n# models = {'EfficientNetB7':model_effnet_B7(drop_rate = 0.1, opt = 'nadam')}\n    \nhistories={}\npredictions={}\npredictions_val={}\npredictions_prob={} #this will be used to give predictions for the ensemble\n# times={}\n\nMODEL_COUNT = len(models)\n\nprint('We have {} models'.format(str(MODEL_COUNT)))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:45:33.384422Z","iopub.execute_input":"2021-08-12T03:45:33.385033Z","iopub.status.idle":"2021-08-12T03:46:54.675165Z","shell.execute_reply.started":"2021-08-12T03:45:33.384987Z","shell.execute_reply":"2021-08-12T03:46:54.673716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SOME NOTES (first trainable run)\n- VGG16: model is HEAVILY overfitting, need to add high dropout rate for this. Something like 0.4 instead of 0.1. training accuracy=0.9733, validation accuracy=0.7489.\n- Xception: model is overfitting, training accuracy reached almost 1 while validation accuracy was stuck around 0.92. \n- DenseNet: model is learning very rapidly, reached training accuracy = 0.99 and validation accuracy = 0.92 by the 9th epoch. There is a slight overfit here, I need to increase drop rate to 0.25. It capped off at a training accuracy of 1 with a validation accuracy of 0.93. \n- EfficientNet: reached 1 training accuracy with 0.94 validation accuracy. Increasing drop rate","metadata":{}},{"cell_type":"markdown","source":"#### A Decision (second trainable run)\nI have decided to remove VGG16 from the running. The model doesn't hold up very well to the others when considering how much more it needs to train to reach a lower accuracy than the others. ","metadata":{}},{"cell_type":"markdown","source":"# Fitting the Models\nWe will fit the models as per the method used in [this notebook](https://www.kaggle.com/servietsky/pretrained-cnn-epic-fight/notebook#5.-Transfer-Learning-)\n\n[This page](https://www.tensorflow.org/tutorials/distribute/custom_training) has crucial information about how this is done.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 12\n\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES//BATCH_SIZE\nds_test_ordered = get_test_dataset(ordered = True)\n\nds_test_images = ds_test_ordered.map(lambda image, idnum: image)\nds_test_idnums = ds_test_ordered.map(lambda image, idnum: idnum)\n\n# import gc\n\n# We initialize our models here:\n# for name, model_ in models.items():\n#     model = model_()\nfor name, model in models.items():\n#     model = model\n    tf.keras.utils.plot_model(model, to_file=name+'.png', show_shapes=True)\n    \n    print('Training Model --- ' + name)\n    history = model.fit(\n        ds_train,\n        steps_per_epoch = STEPS_PER_EPOCH,\n        epochs = EPOCHS,\n        callbacks = [lr_callback],\n        validation_data = ds_valid\n    )\n    \n    histories[name] = history #saving the history\n    predictions_val[name] = np.argmax(model.predict(ds_valid), axis=-1)\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:46:54.67676Z","iopub.execute_input":"2021-08-12T03:46:54.677113Z","iopub.status.idle":"2021-08-12T04:03:54.852241Z","shell.execute_reply.started":"2021-08-12T03:46:54.67708Z","shell.execute_reply":"2021-08-12T04:03:54.85104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can display training metrics using our earlier defined function `display_training_curves`.","metadata":{}},{"cell_type":"code","source":"# Plotting Training curves for model Xception\nmodelname = 'Xception'\ndisplay_training_curves(\n    histories[modelname].history['loss'],\n    histories[modelname].history['val_loss'],\n    'loss',\n    211,\n)\ndisplay_training_curves(\n    histories[modelname].history['sparse_categorical_accuracy'],\n    histories[modelname].history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:54.853923Z","iopub.execute_input":"2021-08-12T04:03:54.854288Z","iopub.status.idle":"2021-08-12T04:03:55.408006Z","shell.execute_reply.started":"2021-08-12T04:03:54.854235Z","shell.execute_reply":"2021-08-12T04:03:55.40691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Training curves for model DenseNet201\nmodelname = 'DenseNet201'\ndisplay_training_curves(\n    histories[modelname].history['loss'],\n    histories[modelname].history['val_loss'],\n    'loss',\n    211,\n)\ndisplay_training_curves(\n    histories[modelname].history['sparse_categorical_accuracy'],\n    histories[modelname].history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:55.40953Z","iopub.execute_input":"2021-08-12T04:03:55.410141Z","iopub.status.idle":"2021-08-12T04:03:55.84492Z","shell.execute_reply.started":"2021-08-12T04:03:55.410091Z","shell.execute_reply":"2021-08-12T04:03:55.843862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Training curves for model EfficientNetB7\nmodelname = 'EfficientNetB7'\ndisplay_training_curves(\n    histories[modelname].history['loss'],\n    histories[modelname].history['val_loss'],\n    'loss',\n    211,\n)\ndisplay_training_curves(\n    histories[modelname].history['sparse_categorical_accuracy'],\n    histories[modelname].history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:55.846464Z","iopub.execute_input":"2021-08-12T04:03:55.847043Z","iopub.status.idle":"2021-08-12T04:03:56.284518Z","shell.execute_reply.started":"2021-08-12T04:03:55.846995Z","shell.execute_reply":"2021-08-12T04:03:56.283432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating Predictions\n\nWe will build a **confusion matrix** on our validation data to evaluate the efficacy of our model. The important metrics here will be **precision**, **recall**, and the **f1-score** which all indicate the predictive value of our model.","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:56.286154Z","iopub.execute_input":"2021-08-12T04:03:56.286809Z","iopub.status.idle":"2021-08-12T04:03:56.301079Z","shell.execute_reply.started":"2021-08-12T04:03:56.286756Z","shell.execute_reply":"2021-08-12T04:03:56.30012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the confusion matrix","metadata":{}},{"cell_type":"code","source":"# cmdataset = get_validation_dataset(ordered = True)\ncmdataset = ds_valid\n# images_ds = cmdataset.map(lambda image, label: image) #simply projecting out the image\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n# cm_probabilities = model.predict(images_ds)\n# cm_predictions = np.argmax(cm_probabilities, axis = -1)# pass this in per model you wish to see\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:56.302335Z","iopub.execute_input":"2021-08-12T04:03:56.302857Z","iopub.status.idle":"2021-08-12T04:03:56.441084Z","shell.execute_reply.started":"2021-08-12T04:03:56.302804Z","shell.execute_reply":"2021-08-12T04:03:56.440022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Investigate the confusion matrix of the model of your choosing below by passing in the name in the dictionary.","metadata":{}},{"cell_type":"code","source":"# model_in_ensemble = 'VGG16'\n# model_in_ensemble = 'Xception'\nmodel_in_ensemble = 'DenseNet201'\n# model_in_ensemble = 'EfficientNetB7'\n\ncm_predictions = predictions_val[model_in_ensemble]\nlabels = range(len(CLASSES))\ncmat = confusion_matrix(\n    cm_correct_labels,\n    cm_predictions,\n    labels = labels\n)\ncmat = (cmat.T/cmat.sum(axis=1)).T #just normalizing the values\nscore = f1_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels = labels,\n    average = 'macro'\n)\nprecision = precision_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels = labels,\n    average = 'macro'\n)\nrecall = recall_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels = labels,\n    average = 'macro'\n)\ndisplay_confusion_matrix(cmat, score, precision, recall)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:03:56.442443Z","iopub.execute_input":"2021-08-12T04:03:56.442741Z","iopub.status.idle":"2021-08-12T04:04:01.408487Z","shell.execute_reply.started":"2021-08-12T04:03:56.442712Z","shell.execute_reply":"2021-08-12T04:04:01.407443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Test Predictions\nNow that we have investigated our three models, let's make predictions using the one we like most (in this case we use the one with the best f1 score).\n\nBased on this metric, we have chosen the DenseNet model since it shows the greatest promise.","metadata":{}},{"cell_type":"code","source":"ds_test_ordered = get_test_dataset(ordered = True)\nds_test_images = ds_test_ordered.map(lambda image, idnum: image)\nds_test_idnums = ds_test_ordered.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(ds_test_idnums.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\nfor name, model in models.items():\n    #Models have already been trained\n    print('Predicting with Model --- ' + name)\n    predictions_prob[name] = model.predict(ds_test_images)\n    predictions[name] = np.argmax(predictions_prob[name], axis=-1)\n    print(predictions)\n\nfinal_model = 'DenseNet201'\nfinal_pred = predictions[final_model]\n\nnp.savetxt(\n    'submission.csv',\n    np.rec.fromarrays([test_ids, final_pred]),\n    fmt=['%s', '%d'],\n    delimiter=',',\n    header='id,label',\n    comments=''\n)\n\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:07:25.922755Z","iopub.execute_input":"2021-08-12T04:07:25.923242Z","iopub.status.idle":"2021-08-12T04:08:53.953036Z","shell.execute_reply.started":"2021-08-12T04:07:25.923209Z","shell.execute_reply":"2021-08-12T04:08:53.951877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}